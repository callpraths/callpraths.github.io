---
layout: post
title:  "Chapter 1: Your problem is not what you think it is"
date:   2022-11-01 00:00:00 +0000
style: geo-feature-study
---

This post is part of a [blog series] on my exploits measuring and squashing reliability woes in the Continuous Integration (CI) actions of the [Azure Communication Services](https://learn.microsoft.com/en-us/azure/communication-services/overview) [web UI library](https://azure.github.io/communication-ui-library/?path=/story/overview--page).

[Azure Communication Services](https://learn.microsoft.com/en-us/azure/communication-services/overview) is a platform you can use to integrate communication services – video and telephony calling, chat and SMS messaging, email etc. – into your applications on the web, mobile or desktop. The offering includes a [web UI library](https://azure.github.io/communication-ui-library/?path=/story/overview--page) that provides a set of UI controls of varying complexity on top of the platform. The UI library handles the complex interactions between UX, the browser and the platform services so that you can focus on branding, custom business logic and deep integration with your business flows.

Web UI library development poses very interesting engineering challenges because of the library’s unique position in the Azure Communication Services ecosystem. First, the library has [complex dependencies](https://github.com/Azure/communication-ui-library/network/dependencies) on the Azure Communication platform because it integrates various platform services developed and released independently. Second, it has a large programmatic and UX contract with client code to support polished branding and frictionless integration with the customers’ applications. Finally, the library has seen [a lot of active development](https://github.com/Azure/communication-ui-library/graphs/contributors?from=2021-01-30&to=2022-09-12&type=c) since its release in late 2021, and will continue to do so as the Azure Communication Services platform grows. The library must hide the platform’s complexity, support a large API surface and consistently maintain a high quality bar, all while [shipping out features at a steady clip](https://www.npmjs.com/package/@azure/communication-react?activeTab=versions) to existing and new customers

It is unnecessary in 2022 to advertise the importance of [Continuous Integration (CI)](https://en.wikipedia.org/wiki/Continuous_integration) given these requirements. The team behind the library recognizes this need. Since the 1.0.0 release, the library is supported by [Github Actions](https://github.com/features/actions) based CI that employs code linters, unit-tests, and [Playwright based browser tests](https://playwright.dev/docs/test-reporters). This early investment in CI has paid off. The team has shipped a minor version of the library roughly monthly through the last year with some major additional features, relying on automation to maintain high quality and low risk. 

The team’s journey with CI automation in the last year hasn’t been all rainbows and sunshine. In March 2022, about 4 months and a few minor releases after the 1.0.0 release, the outlook of developers on the team towards CI was lukewarm. Developers were paying a high cost for CI because of increased friction from slow CI builds and there were internal debates on the cost-benefit ratio of CI considering this friction. I took a first stab at this problem in a [Microsoft-internal Hackathon](https://news.microsoft.com/life/hackathon/). I was able to demonstrate a reduction in the total machine-time consumed by a CI run of 38% by switching from [webpack](https://webpack.js.org/) to [esbuild](https://esbuild.github.io/) for building sample applications. But the overall latency of CI is what matters in a [pre-submit CI](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/collaborating-on-repositories-with-code-quality-features/about-status-checks) – it is the minimum time taken to complete a developer’s request to [merge](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/incorporating-changes-from-a-pull-request/merging-a-pull-request) an approved [Pull Request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests). I was only able to achieve an improvement of 11% in CI latency, and it became apparent that the next bottleneck was the time taken to execute the Playwright tests.

I was not really surprised by this latency bottleneck. At the time, browser test execution involved launching a modified Chromium browser, loading a sample application in the browser, and walking through key user journeys in the application. Typical tests sent chat messages and joined video calls against live Azure Communication Services backend services. There were over 60 browser tests in the test matrix and they had to be run sequentially to avoid failures due to [API throttling from backend services](https://learn.microsoft.com/en-us/partner-center/develop/api-throttling-guidance). The result: a slow and potentially fragile CI that was likely to get worse as more features (and tests!) were added to the library.

I had previously experimented with replacing some of the backend dependencies for browser tests with [in-memory fake implementations](https://github.com/Azure/communication-ui-library/tree/prprabhu/fake-chat-client/packages/fake-backends) and, in July 2022, the team decided that it was time to speed up browser tests by replacing more backend services with fakes. Before I began, I wanted to know the exit criteria – current and target test runtimes for a successful conclusion of the effort. As it turned out, I did not have a good idea how slow the tests ran and needed to do some legwork before I could get a satisfactory estimate.

I had a clear quantitative question to answer. It was time to reach for some data. Unfortunately, I found that GitHub does not include a mature CI analytics feature set (like that of [Azure Pipelines](https://devblogs.microsoft.com/devops/test-analytics-now-at-your-fingertips/)). I instead built a low-tech data-analytics pipeline myself that allowed me to experiment fast at the cost of some additional toil:

- I [enabled](https://github.com/Azure/communication-ui-library/pull/2025) a [Playwright reporter](https://playwright.dev/docs/test-reporters#json-reporter) in CI to generate test statistics at the end of a test run and [uploaded the statistics as a CI artifact](https://docs.github.com/en/actions/using-workflows/storing-workflow-data-as-artifacts).
- In a [separate GitHub repository](https://github.com/prprabhu-ms/acr-e2e-analysis),
  - I wrote a [Go program]() to fetch the uploaded test artifact and Github Actions metadata using [GitHub REST API](https://docs.github.com/en/rest) (I also explored [GitHub GraphQL API](https://docs.github.com/en/graphql), but found that it would not have made a material difference to the effort required).
  - I wrote a [series of Jupyter notebooks](https://github.com/prprabhu-ms/acr-e2e-analysis/tree/main/analysis) to process and analyze the fetched data.

This approach allowed me to focus on the analysis rather than investing too much engineering effort in building a new robust data pipeline from scratch. The use of a GitHub repository allowed me to treat my data analysis experiment as code, with a number of [refactoring passes](https://github.com/prprabhu-ms/acr-e2e-analysis/commit/ec4beace8709829383dd45d5bdb744820d133254), [bug fixes](https://github.com/prprabhu-ms/acr-e2e-analysis/commit/5e0db468a84e769be91670f59787a8c5f42a5f63), [toil reduction automation](https://github.com/prprabhu-ms/acr-e2e-analysis/commit/663409614fab1b450c3b516bc35bacd9f506067a) and [cheap one-off analyses](https://github.com/prprabhu-ms/acr-e2e-analysis/commit/1036a191f9ca5a744a1ac529b44728fdb63b0f92). On the downside, the (incrementally) fetched data soon got large enough that I had to move the data files [GitHub’s Git Large File Storage](https://git-lfs.github.com/).

At the end of a couple weeks of effort, I had reasonable confidence in the quality of fetched data and some preliminary analysis of runtime:
